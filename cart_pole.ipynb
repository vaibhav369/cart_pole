{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cart_pole.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"3Tv3bjw1LK1G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"outputId":"77872a40-1115-4f50-a338-e0c0185b8e6e","executionInfo":{"status":"ok","timestamp":1548690979148,"user_tz":-330,"elapsed":7738,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["!pip install keras-rl"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting keras-rl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n","\u001b[K    100% |████████████████████████████████| 40kB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.5)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.6)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.6)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n","Building wheels for collected packages: keras-rl\n","  Running setup.py bdist_wheel for keras-rl ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n","Successfully built keras-rl\n","Installing collected packages: keras-rl\n","Successfully installed keras-rl-0.4.2\n"],"name":"stdout"}]},{"metadata":{"id":"ZrNzwmitKV1I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"95bf949d-67a4-4afe-f15c-65fd3b75592f","executionInfo":{"status":"ok","timestamp":1548690988315,"user_tz":-330,"elapsed":4345,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["import gym\n","import numpy as np\n","import os\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from keras.optimizers import Adam\n","\n","from rl.agents import DQNAgent\n","from rl.policy import BoltzmannQPolicy, GreedyQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy, BoltzmannGumbelQPolicy\n","from rl.memory import SequentialMemory"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"9FITJ4trLDQR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"b33684d9-d8f5-404a-8c6f-9c2f3ce23a5e","executionInfo":{"status":"ok","timestamp":1548691158566,"user_tz":-330,"elapsed":876,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["ENV_NAME = 'CartPole-v1'\n","\n","env = gym.make(ENV_NAME)\n","obs_space_shape = env.observation_space.shape\n","nb_actions = env.action_space.n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"],"name":"stderr"}]},{"metadata":{"id":"2qWuu6lnL7I9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":454},"outputId":"a7f8c1fb-a635-4c92-a0f4-00fd0e12d19f","executionInfo":{"status":"ok","timestamp":1548691357424,"user_tz":-330,"elapsed":940,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["model = Sequential()\n","model.add( Flatten(input_shape=(1, ) + obs_space_shape) )\n","model.add( Dense(16) )\n","model.add( Activation('relu') )\n","model.add( Dense(16) )\n","model.add( Activation('relu') )\n","model.add( Dense(16) )\n","model.add( Activation('relu') )\n","model.add( Dense(nb_actions) )\n","model.add( Activation('linear') )\n","\n","print(model.summary())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 4)                 0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 16)                80        \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 16)                272       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 16)                272       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 16)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 2)                 34        \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 2)                 0         \n","=================================================================\n","Total params: 658\n","Trainable params: 658\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"metadata":{"id":"zlR0MCUKMedx","colab_type":"code","colab":{}},"cell_type":"code","source":["memory = SequentialMemory(limit=50000, window_length=1)\n","policy = BoltzmannQPolicy()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W6qeBWiYcSpI","colab_type":"code","colab":{}},"cell_type":"code","source":["dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=nb_actions,\n","               nb_steps_warmup=10, target_model_update=1e-2)\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PNs5O1NkcjQg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":6992},"outputId":"13341fd6-c9fd-47c8-e1c2-8ca00021492f","executionInfo":{"status":"ok","timestamp":1548696372820,"user_tz":-330,"elapsed":674054,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["history = dqn.fit(env, nb_steps=80000, visualize=False, verbose=2)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training for 80000 steps ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"],"name":"stderr"},{"output_type":"stream","text":["    57/80000: episode: 1, duration: 1.806s, episode steps: 57, steps per second: 32, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.019 [-1.367, 0.776], loss: 0.417743, mean_absolute_error: 0.515807, mean_q: 0.209721\n","    77/80000: episode: 2, duration: 0.164s, episode steps: 20, steps per second: 122, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.180, 0.773], loss: 0.226631, mean_absolute_error: 0.522589, mean_q: 0.575864\n","   110/80000: episode: 3, duration: 0.269s, episode steps: 33, steps per second: 123, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.082 [-0.583, 1.538], loss: 0.074939, mean_absolute_error: 0.595167, mean_q: 1.036976\n","   127/80000: episode: 4, duration: 0.139s, episode steps: 17, steps per second: 122, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.080 [-1.825, 1.036], loss: 0.041143, mean_absolute_error: 0.702456, mean_q: 1.309836\n","   146/80000: episode: 5, duration: 0.166s, episode steps: 19, steps per second: 115, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.066 [-1.335, 2.190], loss: 0.041736, mean_absolute_error: 0.746583, mean_q: 1.322782\n","   173/80000: episode: 6, duration: 0.218s, episode steps: 27, steps per second: 124, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.101 [-1.668, 0.624], loss: 0.039296, mean_absolute_error: 0.839408, mean_q: 1.548206\n","   188/80000: episode: 7, duration: 0.125s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.072 [-0.979, 1.531], loss: 0.026908, mean_absolute_error: 0.889340, mean_q: 1.672692\n","   199/80000: episode: 8, duration: 0.095s, episode steps: 11, steps per second: 116, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.107 [-2.404, 1.601], loss: 0.048518, mean_absolute_error: 0.970845, mean_q: 1.836990\n","   232/80000: episode: 9, duration: 0.274s, episode steps: 33, steps per second: 121, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.086 [-1.701, 0.978], loss: 0.050458, mean_absolute_error: 1.038795, mean_q: 1.959739\n","   246/80000: episode: 10, duration: 0.119s, episode steps: 14, steps per second: 118, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.088 [-0.826, 1.316], loss: 0.051322, mean_absolute_error: 1.114302, mean_q: 2.133762\n","   260/80000: episode: 11, duration: 0.143s, episode steps: 14, steps per second: 98, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.079 [-1.998, 3.013], loss: 0.040896, mean_absolute_error: 1.175037, mean_q: 2.335034\n","   274/80000: episode: 12, duration: 0.129s, episode steps: 14, steps per second: 108, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-1.011, 1.729], loss: 0.049817, mean_absolute_error: 1.254000, mean_q: 2.472045\n","   287/80000: episode: 13, duration: 0.124s, episode steps: 13, steps per second: 105, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.113 [-1.166, 1.870], loss: 0.076443, mean_absolute_error: 1.330760, mean_q: 2.586453\n","   308/80000: episode: 14, duration: 0.184s, episode steps: 21, steps per second: 114, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.052 [-1.407, 0.838], loss: 0.092566, mean_absolute_error: 1.405605, mean_q: 2.737338\n","   327/80000: episode: 15, duration: 0.176s, episode steps: 19, steps per second: 108, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.110 [-0.996, 2.000], loss: 0.100276, mean_absolute_error: 1.486635, mean_q: 2.865299\n","   343/80000: episode: 16, duration: 0.144s, episode steps: 16, steps per second: 111, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.074 [-1.658, 1.001], loss: 0.100251, mean_absolute_error: 1.572180, mean_q: 3.006151\n","   366/80000: episode: 17, duration: 0.205s, episode steps: 23, steps per second: 112, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.066 [-1.566, 0.838], loss: 0.103850, mean_absolute_error: 1.655818, mean_q: 3.214821\n","   379/80000: episode: 18, duration: 0.128s, episode steps: 13, steps per second: 102, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.101 [-2.265, 1.356], loss: 0.145328, mean_absolute_error: 1.739991, mean_q: 3.366257\n","   388/80000: episode: 19, duration: 0.085s, episode steps: 9, steps per second: 106, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.128 [-0.977, 1.602], loss: 0.170263, mean_absolute_error: 1.779840, mean_q: 3.388337\n","   400/80000: episode: 20, duration: 0.113s, episode steps: 12, steps per second: 106, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.134 [-3.034, 1.925], loss: 0.137724, mean_absolute_error: 1.847533, mean_q: 3.582110\n","   423/80000: episode: 21, duration: 0.205s, episode steps: 23, steps per second: 112, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.056 [-0.992, 1.828], loss: 0.195693, mean_absolute_error: 1.922740, mean_q: 3.664042\n","   433/80000: episode: 22, duration: 0.092s, episode steps: 10, steps per second: 109, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.134 [-1.590, 2.612], loss: 0.139426, mean_absolute_error: 1.994234, mean_q: 3.888987\n","   443/80000: episode: 23, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-1.983, 1.201], loss: 0.201491, mean_absolute_error: 2.025010, mean_q: 3.895056\n","   472/80000: episode: 24, duration: 0.261s, episode steps: 29, steps per second: 111, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.151 [-0.976, 0.609], loss: 0.184224, mean_absolute_error: 2.090741, mean_q: 4.003796\n","   494/80000: episode: 25, duration: 0.201s, episode steps: 22, steps per second: 109, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.063 [-1.025, 1.808], loss: 0.170084, mean_absolute_error: 2.202327, mean_q: 4.209318\n","   511/80000: episode: 26, duration: 0.153s, episode steps: 17, steps per second: 111, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.083 [-1.419, 2.312], loss: 0.161743, mean_absolute_error: 2.260558, mean_q: 4.341311\n","   530/80000: episode: 27, duration: 0.169s, episode steps: 19, steps per second: 113, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.092 [-0.947, 1.800], loss: 0.190863, mean_absolute_error: 2.327222, mean_q: 4.449436\n","   546/80000: episode: 28, duration: 0.143s, episode steps: 16, steps per second: 112, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.969, 1.445], loss: 0.283096, mean_absolute_error: 2.392089, mean_q: 4.456210\n","   559/80000: episode: 29, duration: 0.123s, episode steps: 13, steps per second: 106, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.091 [-1.751, 2.774], loss: 0.301612, mean_absolute_error: 2.457700, mean_q: 4.565495\n","   627/80000: episode: 30, duration: 0.590s, episode steps: 68, steps per second: 115, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.080 [-1.399, 1.775], loss: 0.252824, mean_absolute_error: 2.598665, mean_q: 4.885623\n","   653/80000: episode: 31, duration: 0.226s, episode steps: 26, steps per second: 115, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.004 [-1.567, 2.256], loss: 0.278829, mean_absolute_error: 2.755702, mean_q: 5.212863\n","   667/80000: episode: 32, duration: 0.124s, episode steps: 14, steps per second: 112, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.104 [-1.136, 1.847], loss: 0.217329, mean_absolute_error: 2.824859, mean_q: 5.400075\n","   686/80000: episode: 33, duration: 0.172s, episode steps: 19, steps per second: 110, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.058 [-0.819, 1.471], loss: 0.301656, mean_absolute_error: 2.870738, mean_q: 5.429183\n","   701/80000: episode: 34, duration: 0.135s, episode steps: 15, steps per second: 111, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.120 [-0.563, 1.063], loss: 0.247398, mean_absolute_error: 2.932326, mean_q: 5.664081\n","   728/80000: episode: 35, duration: 0.247s, episode steps: 27, steps per second: 109, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.065 [-0.923, 1.417], loss: 0.314743, mean_absolute_error: 3.056957, mean_q: 5.856177\n","   759/80000: episode: 36, duration: 0.267s, episode steps: 31, steps per second: 116, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.031 [-1.403, 2.365], loss: 0.296161, mean_absolute_error: 3.133347, mean_q: 6.007356\n","   776/80000: episode: 37, duration: 0.146s, episode steps: 17, steps per second: 117, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.066 [-1.183, 1.946], loss: 0.198792, mean_absolute_error: 3.229967, mean_q: 6.267501\n","   800/80000: episode: 38, duration: 0.213s, episode steps: 24, steps per second: 113, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.075 [-1.665, 0.765], loss: 0.263682, mean_absolute_error: 3.333276, mean_q: 6.420930\n","   832/80000: episode: 39, duration: 0.285s, episode steps: 32, steps per second: 112, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.610, 1.210], loss: 0.220207, mean_absolute_error: 3.421901, mean_q: 6.633451\n","   853/80000: episode: 40, duration: 0.186s, episode steps: 21, steps per second: 113, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.118 [-0.565, 0.970], loss: 0.284915, mean_absolute_error: 3.541635, mean_q: 6.877614\n","   889/80000: episode: 41, duration: 0.312s, episode steps: 36, steps per second: 115, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.064 [-1.328, 1.349], loss: 0.349812, mean_absolute_error: 3.656463, mean_q: 7.070650\n","   911/80000: episode: 42, duration: 0.191s, episode steps: 22, steps per second: 115, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.076 [-0.836, 1.723], loss: 0.343642, mean_absolute_error: 3.780228, mean_q: 7.351480\n","   942/80000: episode: 43, duration: 0.286s, episode steps: 31, steps per second: 108, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.092 [-0.767, 1.522], loss: 0.324490, mean_absolute_error: 3.846551, mean_q: 7.461561\n","   961/80000: episode: 44, duration: 0.168s, episode steps: 19, steps per second: 113, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.092 [-0.600, 1.252], loss: 0.350911, mean_absolute_error: 3.940818, mean_q: 7.663817\n","   977/80000: episode: 45, duration: 0.145s, episode steps: 16, steps per second: 111, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.100 [-0.620, 1.287], loss: 0.378390, mean_absolute_error: 4.034569, mean_q: 7.815776\n","   991/80000: episode: 46, duration: 0.122s, episode steps: 14, steps per second: 115, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.108 [-0.807, 1.516], loss: 0.302384, mean_absolute_error: 4.122092, mean_q: 8.095899\n","  1017/80000: episode: 47, duration: 0.229s, episode steps: 26, steps per second: 114, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.060 [-1.008, 0.637], loss: 0.266053, mean_absolute_error: 4.165086, mean_q: 8.157485\n","  1063/80000: episode: 48, duration: 0.402s, episode steps: 46, steps per second: 114, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.084 [-1.004, 0.546], loss: 0.384202, mean_absolute_error: 4.295013, mean_q: 8.405567\n","  1101/80000: episode: 49, duration: 0.330s, episode steps: 38, steps per second: 115, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.346, 0.589], loss: 0.434792, mean_absolute_error: 4.452531, mean_q: 8.734647\n","  1163/80000: episode: 50, duration: 0.528s, episode steps: 62, steps per second: 117, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.117 [-0.817, 1.248], loss: 0.399814, mean_absolute_error: 4.700562, mean_q: 9.223217\n","  1241/80000: episode: 51, duration: 0.665s, episode steps: 78, steps per second: 117, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.272 [-0.784, 1.309], loss: 0.364832, mean_absolute_error: 4.994743, mean_q: 9.914600\n","  1295/80000: episode: 52, duration: 0.467s, episode steps: 54, steps per second: 116, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.051 [-1.501, 1.610], loss: 0.558461, mean_absolute_error: 5.278367, mean_q: 10.453958\n","  1351/80000: episode: 53, duration: 0.485s, episode steps: 56, steps per second: 115, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.062 [-1.673, 1.522], loss: 0.492571, mean_absolute_error: 5.493869, mean_q: 10.919335\n","  1431/80000: episode: 54, duration: 0.683s, episode steps: 80, steps per second: 117, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.267 [-0.962, 1.721], loss: 0.517381, mean_absolute_error: 5.767058, mean_q: 11.515770\n","  1493/80000: episode: 55, duration: 0.524s, episode steps: 62, steps per second: 118, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.090 [-1.424, 0.982], loss: 0.704525, mean_absolute_error: 6.098215, mean_q: 12.145421\n","  1609/80000: episode: 56, duration: 0.983s, episode steps: 116, steps per second: 118, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.198 [-0.827, 1.481], loss: 0.597378, mean_absolute_error: 6.449295, mean_q: 12.955017\n","  1675/80000: episode: 57, duration: 0.563s, episode steps: 66, steps per second: 117, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.201 [-1.822, 1.207], loss: 0.537487, mean_absolute_error: 6.899829, mean_q: 13.970716\n","  1777/80000: episode: 58, duration: 0.871s, episode steps: 102, steps per second: 117, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.092 [-1.086, 1.170], loss: 0.632084, mean_absolute_error: 7.241441, mean_q: 14.692139\n","  1865/80000: episode: 59, duration: 0.753s, episode steps: 88, steps per second: 117, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.128 [-1.286, 1.279], loss: 0.813549, mean_absolute_error: 7.681845, mean_q: 15.523081\n","  1943/80000: episode: 60, duration: 0.663s, episode steps: 78, steps per second: 118, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.184 [-1.119, 0.897], loss: 0.722450, mean_absolute_error: 8.005704, mean_q: 16.221470\n","  2020/80000: episode: 61, duration: 0.666s, episode steps: 77, steps per second: 116, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.002 [-1.536, 1.844], loss: 0.895160, mean_absolute_error: 8.384867, mean_q: 16.943558\n","  2091/80000: episode: 62, duration: 0.599s, episode steps: 71, steps per second: 118, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.169 [-1.256, 0.867], loss: 0.929086, mean_absolute_error: 8.691631, mean_q: 17.591396\n","  2191/80000: episode: 63, duration: 0.845s, episode steps: 100, steps per second: 118, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.212 [-1.467, 1.137], loss: 0.828466, mean_absolute_error: 9.005994, mean_q: 18.268719\n","  2294/80000: episode: 64, duration: 0.866s, episode steps: 103, steps per second: 119, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.192 [-1.857, 1.820], loss: 0.877012, mean_absolute_error: 9.446097, mean_q: 19.162743\n","  2485/80000: episode: 65, duration: 1.596s, episode steps: 191, steps per second: 120, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.154 [-1.914, 1.299], loss: 0.967271, mean_absolute_error: 10.105415, mean_q: 20.600634\n","  2602/80000: episode: 66, duration: 0.996s, episode steps: 117, steps per second: 117, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.251 [-0.768, 1.520], loss: 1.420062, mean_absolute_error: 10.755993, mean_q: 21.834215\n","  2755/80000: episode: 67, duration: 1.304s, episode steps: 153, steps per second: 117, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.256 [-1.860, 1.001], loss: 1.286432, mean_absolute_error: 11.409689, mean_q: 23.187229\n","  2938/80000: episode: 68, duration: 1.542s, episode steps: 183, steps per second: 119, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.338 [-2.613, 1.004], loss: 1.275917, mean_absolute_error: 11.989717, mean_q: 24.427265\n","  3178/80000: episode: 69, duration: 2.041s, episode steps: 240, steps per second: 118, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.230 [-2.108, 0.704], loss: 1.659413, mean_absolute_error: 12.941105, mean_q: 26.392986\n","  3400/80000: episode: 70, duration: 1.873s, episode steps: 222, steps per second: 118, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.245 [-2.071, 1.044], loss: 1.594522, mean_absolute_error: 13.883691, mean_q: 28.333754\n","  3562/80000: episode: 71, duration: 1.382s, episode steps: 162, steps per second: 117, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.355 [-2.239, 0.795], loss: 1.765961, mean_absolute_error: 14.692069, mean_q: 30.048962\n","  3760/80000: episode: 72, duration: 1.722s, episode steps: 198, steps per second: 115, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.172 [-0.662, 1.647], loss: 1.909594, mean_absolute_error: 15.439456, mean_q: 31.469599\n","  3944/80000: episode: 73, duration: 1.590s, episode steps: 184, steps per second: 116, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.353 [-2.413, 1.093], loss: 1.916103, mean_absolute_error: 16.313934, mean_q: 33.342434\n","  4176/80000: episode: 74, duration: 1.957s, episode steps: 232, steps per second: 119, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.255 [-2.178, 0.769], loss: 1.537782, mean_absolute_error: 17.228184, mean_q: 35.197777\n","  4310/80000: episode: 75, duration: 1.137s, episode steps: 134, steps per second: 118, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.309 [-0.701, 1.866], loss: 1.973825, mean_absolute_error: 17.872423, mean_q: 36.535923\n","  4475/80000: episode: 76, duration: 1.389s, episode steps: 165, steps per second: 119, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.280 [-0.788, 2.155], loss: 2.402835, mean_absolute_error: 18.618187, mean_q: 37.953018\n","  4829/80000: episode: 77, duration: 2.993s, episode steps: 354, steps per second: 118, episode reward: 354.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.113 [-0.972, 1.732], loss: 2.285863, mean_absolute_error: 19.712154, mean_q: 40.174023\n","  4966/80000: episode: 78, duration: 1.183s, episode steps: 137, steps per second: 116, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.287 [-0.681, 1.893], loss: 2.638726, mean_absolute_error: 20.852247, mean_q: 42.498188\n","  5159/80000: episode: 79, duration: 1.630s, episode steps: 193, steps per second: 118, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.188 [-0.695, 1.920], loss: 2.398196, mean_absolute_error: 21.295782, mean_q: 43.351440\n","  5329/80000: episode: 80, duration: 1.433s, episode steps: 170, steps per second: 119, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.259 [-0.784, 2.022], loss: 2.442642, mean_absolute_error: 22.102457, mean_q: 45.022560\n","  5497/80000: episode: 81, duration: 1.417s, episode steps: 168, steps per second: 119, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.362 [-0.732, 2.613], loss: 2.698467, mean_absolute_error: 22.570290, mean_q: 45.928310\n","  5650/80000: episode: 82, duration: 1.299s, episode steps: 153, steps per second: 118, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.252 [-0.820, 2.299], loss: 2.433979, mean_absolute_error: 23.128748, mean_q: 47.062645\n","  5947/80000: episode: 83, duration: 2.520s, episode steps: 297, steps per second: 118, episode reward: 297.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.226 [-2.442, 1.011], loss: 2.822661, mean_absolute_error: 23.973639, mean_q: 48.787552\n","  6133/80000: episode: 84, duration: 1.583s, episode steps: 186, steps per second: 118, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.349 [-2.367, 0.825], loss: 2.842426, mean_absolute_error: 24.797699, mean_q: 50.497753\n","  6293/80000: episode: 85, duration: 1.357s, episode steps: 160, steps per second: 118, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.275 [-0.732, 1.819], loss: 2.982863, mean_absolute_error: 25.364450, mean_q: 51.590782\n","  6526/80000: episode: 86, duration: 1.956s, episode steps: 233, steps per second: 119, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.272 [-2.292, 1.070], loss: 2.478035, mean_absolute_error: 25.956160, mean_q: 52.895184\n","  6722/80000: episode: 87, duration: 1.667s, episode steps: 196, steps per second: 118, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.210 [-0.876, 1.756], loss: 2.680502, mean_absolute_error: 26.667870, mean_q: 54.365364\n","  6856/80000: episode: 88, duration: 1.149s, episode steps: 134, steps per second: 117, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.327 [-0.684, 1.705], loss: 3.236365, mean_absolute_error: 26.993559, mean_q: 55.141323\n","  6999/80000: episode: 89, duration: 1.211s, episode steps: 143, steps per second: 118, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.249 [-2.763, 2.843], loss: 2.592330, mean_absolute_error: 27.748419, mean_q: 56.582081\n","  7174/80000: episode: 90, duration: 1.489s, episode steps: 175, steps per second: 118, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.261 [-2.637, 2.853], loss: 2.847553, mean_absolute_error: 28.321936, mean_q: 57.557487\n","  7288/80000: episode: 91, duration: 0.968s, episode steps: 114, steps per second: 118, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.350 [-0.904, 2.756], loss: 3.155178, mean_absolute_error: 28.563072, mean_q: 58.091076\n","  7466/80000: episode: 92, duration: 1.511s, episode steps: 178, steps per second: 118, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.361 [-2.366, 1.019], loss: 3.771857, mean_absolute_error: 28.892616, mean_q: 58.632252\n","  7610/80000: episode: 93, duration: 1.215s, episode steps: 144, steps per second: 119, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.330 [-0.883, 1.773], loss: 3.348605, mean_absolute_error: 29.371874, mean_q: 59.676174\n","  7894/80000: episode: 94, duration: 2.446s, episode steps: 284, steps per second: 116, episode reward: 284.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.186 [-2.862, 3.380], loss: 3.270630, mean_absolute_error: 30.093660, mean_q: 61.022049\n","  8052/80000: episode: 95, duration: 1.358s, episode steps: 158, steps per second: 116, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.308 [-0.934, 1.789], loss: 3.271199, mean_absolute_error: 30.505116, mean_q: 61.940907\n","  8194/80000: episode: 96, duration: 1.198s, episode steps: 142, steps per second: 119, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.292 [-0.686, 1.530], loss: 2.983445, mean_absolute_error: 31.133514, mean_q: 63.165859\n","  8412/80000: episode: 97, duration: 1.850s, episode steps: 218, steps per second: 118, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.249 [-0.906, 1.977], loss: 3.121609, mean_absolute_error: 31.167145, mean_q: 63.402245\n","  8602/80000: episode: 98, duration: 1.622s, episode steps: 190, steps per second: 117, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.333 [-0.860, 2.259], loss: 3.145829, mean_absolute_error: 31.952776, mean_q: 64.847099\n","  8744/80000: episode: 99, duration: 1.214s, episode steps: 142, steps per second: 117, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.326 [-0.883, 1.689], loss: 2.424734, mean_absolute_error: 32.238640, mean_q: 65.657516\n","  8945/80000: episode: 100, duration: 1.715s, episode steps: 201, steps per second: 117, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.326 [-0.816, 2.310], loss: 4.620602, mean_absolute_error: 32.947647, mean_q: 66.844803\n","  9139/80000: episode: 101, duration: 1.646s, episode steps: 194, steps per second: 118, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.348 [-0.974, 2.424], loss: 4.794715, mean_absolute_error: 32.932808, mean_q: 66.744751\n","  9465/80000: episode: 102, duration: 2.776s, episode steps: 326, steps per second: 117, episode reward: 326.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.190 [-2.234, 1.030], loss: 3.296268, mean_absolute_error: 33.901985, mean_q: 68.885338\n","  9647/80000: episode: 103, duration: 1.532s, episode steps: 182, steps per second: 119, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.345 [-0.768, 2.184], loss: 3.044026, mean_absolute_error: 34.683853, mean_q: 70.477890\n","  9862/80000: episode: 104, duration: 1.838s, episode steps: 215, steps per second: 117, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.278 [-2.201, 0.800], loss: 4.094571, mean_absolute_error: 34.714588, mean_q: 70.609642\n"," 10085/80000: episode: 105, duration: 1.902s, episode steps: 223, steps per second: 117, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.301 [-0.827, 2.342], loss: 3.734164, mean_absolute_error: 35.411072, mean_q: 71.964020\n"," 10274/80000: episode: 106, duration: 1.598s, episode steps: 189, steps per second: 118, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.355 [-0.788, 2.403], loss: 4.844677, mean_absolute_error: 35.849148, mean_q: 72.808479\n"," 10449/80000: episode: 107, duration: 1.484s, episode steps: 175, steps per second: 118, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.370 [-0.950, 2.422], loss: 4.118898, mean_absolute_error: 36.474514, mean_q: 74.019798\n"," 10660/80000: episode: 108, duration: 1.788s, episode steps: 211, steps per second: 118, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.312 [-0.870, 2.405], loss: 3.608047, mean_absolute_error: 36.856987, mean_q: 74.777985\n"," 10858/80000: episode: 109, duration: 1.675s, episode steps: 198, steps per second: 118, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.341 [-0.783, 2.406], loss: 4.512685, mean_absolute_error: 36.907455, mean_q: 74.868706\n"," 11036/80000: episode: 110, duration: 1.502s, episode steps: 178, steps per second: 118, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.382 [-0.797, 2.407], loss: 4.002327, mean_absolute_error: 37.442787, mean_q: 75.996925\n"," 11263/80000: episode: 111, duration: 1.901s, episode steps: 227, steps per second: 119, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.289 [-0.823, 2.410], loss: 3.979764, mean_absolute_error: 37.861149, mean_q: 76.748657\n"," 11460/80000: episode: 112, duration: 1.672s, episode steps: 197, steps per second: 118, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.354 [-0.679, 2.403], loss: 4.576546, mean_absolute_error: 38.109272, mean_q: 77.383430\n"," 11636/80000: episode: 113, duration: 1.518s, episode steps: 176, steps per second: 116, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.313 [-2.058, 0.783], loss: 3.383462, mean_absolute_error: 38.509506, mean_q: 78.235489\n"," 11852/80000: episode: 114, duration: 1.849s, episode steps: 216, steps per second: 117, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.317 [-0.817, 2.429], loss: 3.599191, mean_absolute_error: 38.944244, mean_q: 79.084457\n"," 12049/80000: episode: 115, duration: 1.694s, episode steps: 197, steps per second: 116, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.346 [-0.918, 2.408], loss: 3.678274, mean_absolute_error: 39.378197, mean_q: 80.023521\n"," 12276/80000: episode: 116, duration: 1.924s, episode steps: 227, steps per second: 118, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.291 [-0.865, 2.420], loss: 3.312390, mean_absolute_error: 39.825672, mean_q: 80.829063\n"," 12459/80000: episode: 117, duration: 1.547s, episode steps: 183, steps per second: 118, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.372 [-0.783, 2.418], loss: 5.587501, mean_absolute_error: 40.052361, mean_q: 81.096535\n"," 12630/80000: episode: 118, duration: 1.454s, episode steps: 171, steps per second: 118, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.394 [-1.068, 2.414], loss: 4.729334, mean_absolute_error: 40.392925, mean_q: 81.762199\n"," 12819/80000: episode: 119, duration: 1.610s, episode steps: 189, steps per second: 117, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.342 [-0.826, 2.402], loss: 4.259803, mean_absolute_error: 40.522022, mean_q: 82.117325\n"," 13016/80000: episode: 120, duration: 1.670s, episode steps: 197, steps per second: 118, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.340 [-0.919, 2.413], loss: 3.418715, mean_absolute_error: 40.832745, mean_q: 82.731979\n"," 13192/80000: episode: 121, duration: 1.484s, episode steps: 176, steps per second: 119, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.383 [-0.757, 2.418], loss: 4.355217, mean_absolute_error: 41.168247, mean_q: 83.477272\n"," 13407/80000: episode: 122, duration: 1.823s, episode steps: 215, steps per second: 118, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.316 [-1.157, 2.414], loss: 5.051358, mean_absolute_error: 41.575344, mean_q: 84.308891\n"," 13572/80000: episode: 123, duration: 1.398s, episode steps: 165, steps per second: 118, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.390 [-0.940, 2.403], loss: 4.153173, mean_absolute_error: 41.532600, mean_q: 84.292984\n"," 13762/80000: episode: 124, duration: 1.598s, episode steps: 190, steps per second: 119, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.345 [-1.057, 2.421], loss: 5.087141, mean_absolute_error: 42.154945, mean_q: 85.446121\n"," 13979/80000: episode: 125, duration: 1.853s, episode steps: 217, steps per second: 117, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.299 [-0.884, 2.428], loss: 4.707290, mean_absolute_error: 41.943466, mean_q: 84.941765\n"," 14152/80000: episode: 126, duration: 1.512s, episode steps: 173, steps per second: 114, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.380 [-0.744, 2.411], loss: 4.030428, mean_absolute_error: 42.168934, mean_q: 85.439789\n"," 14365/80000: episode: 127, duration: 1.804s, episode steps: 213, steps per second: 118, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.314 [-0.912, 2.408], loss: 4.728095, mean_absolute_error: 42.516418, mean_q: 86.233017\n"," 14578/80000: episode: 128, duration: 1.798s, episode steps: 213, steps per second: 118, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.316 [-0.904, 2.421], loss: 4.936975, mean_absolute_error: 42.768547, mean_q: 86.745323\n"," 14773/80000: episode: 129, duration: 1.646s, episode steps: 195, steps per second: 118, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.332 [-0.848, 2.412], loss: 3.467754, mean_absolute_error: 43.330147, mean_q: 87.760406\n"," 14995/80000: episode: 130, duration: 1.827s, episode steps: 222, steps per second: 122, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.298 [-1.023, 2.436], loss: 5.010990, mean_absolute_error: 43.359207, mean_q: 87.761024\n"," 15200/80000: episode: 131, duration: 1.707s, episode steps: 205, steps per second: 120, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.331 [-0.889, 2.437], loss: 3.868374, mean_absolute_error: 43.372295, mean_q: 87.924774\n"," 15393/80000: episode: 132, duration: 1.621s, episode steps: 193, steps per second: 119, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.298 [-2.206, 0.903], loss: 4.095205, mean_absolute_error: 43.296036, mean_q: 87.738655\n"," 15577/80000: episode: 133, duration: 1.534s, episode steps: 184, steps per second: 120, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.344 [-2.989, 0.917], loss: 4.076580, mean_absolute_error: 43.850300, mean_q: 88.804993\n"," 15854/80000: episode: 134, duration: 2.307s, episode steps: 277, steps per second: 120, episode reward: 277.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.236 [-0.959, 2.428], loss: 3.936567, mean_absolute_error: 44.535542, mean_q: 90.225380\n"," 16034/80000: episode: 135, duration: 1.512s, episode steps: 180, steps per second: 119, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.360 [-0.922, 2.424], loss: 2.845516, mean_absolute_error: 44.104805, mean_q: 89.393860\n"," 16206/80000: episode: 136, duration: 1.441s, episode steps: 172, steps per second: 119, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.385 [-0.724, 2.400], loss: 3.369987, mean_absolute_error: 44.685341, mean_q: 90.368355\n"," 16385/80000: episode: 137, duration: 1.503s, episode steps: 179, steps per second: 119, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.366 [-1.061, 2.420], loss: 3.106465, mean_absolute_error: 44.510315, mean_q: 90.142426\n"," 16609/80000: episode: 138, duration: 1.860s, episode steps: 224, steps per second: 120, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.281 [-3.330, 1.502], loss: 3.854603, mean_absolute_error: 44.569092, mean_q: 90.185501\n"," 16857/80000: episode: 139, duration: 2.092s, episode steps: 248, steps per second: 119, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.249 [-0.910, 2.411], loss: 3.270406, mean_absolute_error: 45.137760, mean_q: 91.359955\n"," 17011/80000: episode: 140, duration: 1.308s, episode steps: 154, steps per second: 118, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.399 [-0.933, 2.420], loss: 2.225562, mean_absolute_error: 44.877220, mean_q: 90.785507\n"," 17170/80000: episode: 141, duration: 1.347s, episode steps: 159, steps per second: 118, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.397 [-2.775, 0.809], loss: 3.884331, mean_absolute_error: 45.491707, mean_q: 92.052002\n"," 17348/80000: episode: 142, duration: 1.512s, episode steps: 178, steps per second: 118, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.326 [-3.718, 2.507], loss: 4.421051, mean_absolute_error: 45.081833, mean_q: 91.172646\n"," 17535/80000: episode: 143, duration: 1.574s, episode steps: 187, steps per second: 119, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.336 [-2.412, 1.082], loss: 4.361198, mean_absolute_error: 45.459774, mean_q: 91.762985\n"," 17715/80000: episode: 144, duration: 1.517s, episode steps: 180, steps per second: 119, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.342 [-0.859, 2.414], loss: 4.581783, mean_absolute_error: 44.922184, mean_q: 90.814247\n"," 17875/80000: episode: 145, duration: 1.344s, episode steps: 160, steps per second: 119, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.378 [-2.305, 0.783], loss: 2.050871, mean_absolute_error: 45.328575, mean_q: 91.693665\n"," 18057/80000: episode: 146, duration: 1.542s, episode steps: 182, steps per second: 118, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.351 [-0.883, 2.439], loss: 3.488537, mean_absolute_error: 44.680832, mean_q: 90.494980\n"," 18264/80000: episode: 147, duration: 1.766s, episode steps: 207, steps per second: 117, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.309 [-0.966, 2.427], loss: 3.076652, mean_absolute_error: 44.950321, mean_q: 90.993851\n"," 18422/80000: episode: 148, duration: 1.349s, episode steps: 158, steps per second: 117, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.380 [-3.392, 1.630], loss: 4.368326, mean_absolute_error: 45.295124, mean_q: 91.689697\n"," 18579/80000: episode: 149, duration: 1.322s, episode steps: 157, steps per second: 119, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.407 [-0.682, 2.433], loss: 2.939127, mean_absolute_error: 46.102100, mean_q: 93.308792\n"," 18793/80000: episode: 150, duration: 1.855s, episode steps: 214, steps per second: 115, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.304 [-0.893, 2.409], loss: 2.934728, mean_absolute_error: 45.298500, mean_q: 91.587181\n"," 19005/80000: episode: 151, duration: 2.114s, episode steps: 212, steps per second: 100, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.296 [-2.946, 0.921], loss: 2.798057, mean_absolute_error: 45.217804, mean_q: 91.441528\n"," 19164/80000: episode: 152, duration: 1.584s, episode steps: 159, steps per second: 100, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.402 [-0.973, 2.404], loss: 3.424954, mean_absolute_error: 44.962082, mean_q: 90.898994\n"," 19352/80000: episode: 153, duration: 1.789s, episode steps: 188, steps per second: 105, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.346 [-1.032, 2.446], loss: 4.113024, mean_absolute_error: 45.406395, mean_q: 91.756729\n"," 19536/80000: episode: 154, duration: 1.552s, episode steps: 184, steps per second: 119, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.347 [-0.913, 2.417], loss: 3.135570, mean_absolute_error: 44.957897, mean_q: 90.927269\n"," 19683/80000: episode: 155, duration: 1.236s, episode steps: 147, steps per second: 119, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.421 [-0.942, 2.404], loss: 2.577413, mean_absolute_error: 44.743095, mean_q: 90.431519\n"," 19840/80000: episode: 156, duration: 1.309s, episode steps: 157, steps per second: 120, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.414 [-0.984, 2.402], loss: 3.058382, mean_absolute_error: 44.745468, mean_q: 90.540245\n"," 20043/80000: episode: 157, duration: 1.697s, episode steps: 203, steps per second: 120, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.318 [-1.138, 2.409], loss: 3.361408, mean_absolute_error: 45.125114, mean_q: 91.101891\n"," 20210/80000: episode: 158, duration: 1.449s, episode steps: 167, steps per second: 115, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.390 [-0.899, 2.429], loss: 2.682692, mean_absolute_error: 45.113636, mean_q: 91.048660\n"," 20447/80000: episode: 159, duration: 2.020s, episode steps: 237, steps per second: 117, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.278 [-1.184, 2.794], loss: 3.837612, mean_absolute_error: 44.836334, mean_q: 90.573723\n"," 20604/80000: episode: 160, duration: 1.328s, episode steps: 157, steps per second: 118, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.409 [-0.908, 2.447], loss: 2.439082, mean_absolute_error: 44.774036, mean_q: 90.482162\n"," 20780/80000: episode: 161, duration: 1.484s, episode steps: 176, steps per second: 119, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.356 [-2.967, 1.007], loss: 2.728065, mean_absolute_error: 45.091396, mean_q: 90.939316\n"," 20919/80000: episode: 162, duration: 1.174s, episode steps: 139, steps per second: 118, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.387 [-2.077, 0.970], loss: 5.650184, mean_absolute_error: 44.791035, mean_q: 90.321365\n"," 21079/80000: episode: 163, duration: 1.385s, episode steps: 160, steps per second: 116, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.403 [-0.799, 2.439], loss: 3.476409, mean_absolute_error: 44.602924, mean_q: 89.919189\n"," 21264/80000: episode: 164, duration: 1.563s, episode steps: 185, steps per second: 118, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.337 [-2.779, 1.058], loss: 3.855567, mean_absolute_error: 44.911282, mean_q: 90.628571\n"," 21425/80000: episode: 165, duration: 1.390s, episode steps: 161, steps per second: 116, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.388 [-2.611, 0.933], loss: 2.744820, mean_absolute_error: 44.559662, mean_q: 90.086380\n"," 21582/80000: episode: 166, duration: 1.376s, episode steps: 157, steps per second: 114, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.371 [-3.581, 2.312], loss: 2.849126, mean_absolute_error: 44.300892, mean_q: 89.592186\n"," 21745/80000: episode: 167, duration: 1.389s, episode steps: 163, steps per second: 117, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.379 [-1.087, 2.764], loss: 3.757694, mean_absolute_error: 44.757511, mean_q: 90.454315\n"," 21925/80000: episode: 168, duration: 1.500s, episode steps: 180, steps per second: 120, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.345 [-3.324, 1.368], loss: 2.252755, mean_absolute_error: 44.802139, mean_q: 90.464516\n"," 22070/80000: episode: 169, duration: 1.207s, episode steps: 145, steps per second: 120, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.377 [-2.097, 1.143], loss: 1.978450, mean_absolute_error: 44.685379, mean_q: 90.286514\n"," 22250/80000: episode: 170, duration: 1.477s, episode steps: 180, steps per second: 122, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.353 [-0.841, 2.421], loss: 3.537728, mean_absolute_error: 44.442020, mean_q: 89.651703\n"," 22440/80000: episode: 171, duration: 1.596s, episode steps: 190, steps per second: 119, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.352 [-0.743, 2.417], loss: 2.812185, mean_absolute_error: 43.874344, mean_q: 88.530327\n"," 22614/80000: episode: 172, duration: 1.459s, episode steps: 174, steps per second: 119, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.320 [-2.166, 1.261], loss: 2.166083, mean_absolute_error: 44.013233, mean_q: 88.878036\n"," 22819/80000: episode: 173, duration: 1.713s, episode steps: 205, steps per second: 120, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.338 [-0.927, 2.454], loss: 3.470799, mean_absolute_error: 43.834393, mean_q: 88.394264\n"," 23005/80000: episode: 174, duration: 1.551s, episode steps: 186, steps per second: 120, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.373 [-1.118, 2.407], loss: 2.804969, mean_absolute_error: 44.044857, mean_q: 88.861382\n"," 23176/80000: episode: 175, duration: 1.421s, episode steps: 171, steps per second: 120, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.305 [-2.013, 0.804], loss: 3.052175, mean_absolute_error: 44.008045, mean_q: 88.902351\n"," 23394/80000: episode: 176, duration: 1.800s, episode steps: 218, steps per second: 121, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.305 [-0.880, 2.424], loss: 2.144327, mean_absolute_error: 44.020527, mean_q: 88.850670\n"," 23590/80000: episode: 177, duration: 1.618s, episode steps: 196, steps per second: 121, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.258 [-1.957, 0.804], loss: 2.222157, mean_absolute_error: 43.936127, mean_q: 88.688217\n"," 23733/80000: episode: 178, duration: 1.197s, episode steps: 143, steps per second: 119, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.349 [-1.993, 0.743], loss: 2.433859, mean_absolute_error: 43.698814, mean_q: 88.101723\n"," 23896/80000: episode: 179, duration: 1.370s, episode steps: 163, steps per second: 119, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.291 [-1.792, 1.231], loss: 2.417644, mean_absolute_error: 43.725449, mean_q: 88.102982\n"," 24028/80000: episode: 180, duration: 1.114s, episode steps: 132, steps per second: 118, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.361 [-1.850, 0.658], loss: 1.887877, mean_absolute_error: 43.496559, mean_q: 87.824348\n"," 24177/80000: episode: 181, duration: 1.267s, episode steps: 149, steps per second: 118, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.303 [-1.726, 0.793], loss: 2.414162, mean_absolute_error: 43.890705, mean_q: 88.408257\n"," 24418/80000: episode: 182, duration: 1.988s, episode steps: 241, steps per second: 121, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.288 [-1.204, 2.405], loss: 2.708784, mean_absolute_error: 43.518204, mean_q: 87.713539\n"," 24617/80000: episode: 183, duration: 1.663s, episode steps: 199, steps per second: 120, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.333 [-1.038, 2.434], loss: 2.694363, mean_absolute_error: 43.481152, mean_q: 87.713753\n"," 24761/80000: episode: 184, duration: 1.198s, episode steps: 144, steps per second: 120, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.330 [-1.805, 0.652], loss: 3.404212, mean_absolute_error: 43.439159, mean_q: 87.681778\n"," 24930/80000: episode: 185, duration: 1.404s, episode steps: 169, steps per second: 120, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.337 [-3.994, 3.035], loss: 3.070493, mean_absolute_error: 43.098072, mean_q: 86.934578\n"," 25090/80000: episode: 186, duration: 1.320s, episode steps: 160, steps per second: 121, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.341 [-4.108, 3.422], loss: 2.932086, mean_absolute_error: 43.073303, mean_q: 86.882164\n"," 25290/80000: episode: 187, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.307 [-3.303, 1.357], loss: 2.017628, mean_absolute_error: 43.179012, mean_q: 87.039841\n"," 25469/80000: episode: 188, duration: 1.518s, episode steps: 179, steps per second: 118, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.349 [-1.184, 2.601], loss: 3.248611, mean_absolute_error: 43.087044, mean_q: 86.849197\n"," 25652/80000: episode: 189, duration: 1.556s, episode steps: 183, steps per second: 118, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.363 [-1.171, 2.431], loss: 1.785016, mean_absolute_error: 42.887722, mean_q: 86.529320\n"," 25829/80000: episode: 190, duration: 1.498s, episode steps: 177, steps per second: 118, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.388 [-1.310, 2.411], loss: 2.375090, mean_absolute_error: 42.274727, mean_q: 85.217720\n"," 26046/80000: episode: 191, duration: 1.829s, episode steps: 217, steps per second: 119, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.300 [-1.183, 2.420], loss: 2.211960, mean_absolute_error: 42.665943, mean_q: 86.017181\n"," 26259/80000: episode: 192, duration: 1.785s, episode steps: 213, steps per second: 119, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.316 [-1.078, 2.407], loss: 1.985963, mean_absolute_error: 42.489960, mean_q: 85.568184\n"," 26421/80000: episode: 193, duration: 1.362s, episode steps: 162, steps per second: 119, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.285 [-1.799, 1.039], loss: 1.403573, mean_absolute_error: 42.090282, mean_q: 84.835144\n"," 26617/80000: episode: 194, duration: 1.653s, episode steps: 196, steps per second: 119, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.341 [-1.421, 2.550], loss: 2.574867, mean_absolute_error: 42.265709, mean_q: 85.151421\n"," 26819/80000: episode: 195, duration: 1.700s, episode steps: 202, steps per second: 119, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.308 [-1.185, 2.606], loss: 2.368001, mean_absolute_error: 42.405468, mean_q: 85.367584\n"," 27036/80000: episode: 196, duration: 1.812s, episode steps: 217, steps per second: 120, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.310 [-1.132, 2.440], loss: 2.002911, mean_absolute_error: 42.285275, mean_q: 85.198349\n"," 27245/80000: episode: 197, duration: 1.757s, episode steps: 209, steps per second: 119, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.291 [-2.168, 3.234], loss: 3.208023, mean_absolute_error: 42.233017, mean_q: 84.964882\n"," 27390/80000: episode: 198, duration: 1.221s, episode steps: 145, steps per second: 119, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.428 [0.000, 1.000], mean observation: -0.368 [-3.964, 3.333], loss: 1.648602, mean_absolute_error: 41.992832, mean_q: 84.572464\n"," 27565/80000: episode: 199, duration: 1.469s, episode steps: 175, steps per second: 119, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.325 [-3.938, 2.771], loss: 1.577020, mean_absolute_error: 41.434258, mean_q: 83.436852\n"," 27805/80000: episode: 200, duration: 2.025s, episode steps: 240, steps per second: 119, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.288 [-1.233, 2.412], loss: 2.092025, mean_absolute_error: 41.677349, mean_q: 84.037971\n"," 27944/80000: episode: 201, duration: 1.167s, episode steps: 139, steps per second: 119, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.275 [-1.664, 0.730], loss: 1.845286, mean_absolute_error: 41.049683, mean_q: 82.784737\n"," 28114/80000: episode: 202, duration: 1.420s, episode steps: 170, steps per second: 120, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.349 [-3.323, 1.602], loss: 1.977497, mean_absolute_error: 41.429222, mean_q: 83.452950\n"," 28313/80000: episode: 203, duration: 1.690s, episode steps: 199, steps per second: 118, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.318 [-1.438, 2.837], loss: 2.131234, mean_absolute_error: 41.225433, mean_q: 83.079834\n"," 28497/80000: episode: 204, duration: 1.580s, episode steps: 184, steps per second: 116, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.332 [-3.718, 1.888], loss: 1.380619, mean_absolute_error: 40.902882, mean_q: 82.346428\n"," 28763/80000: episode: 205, duration: 2.256s, episode steps: 266, steps per second: 118, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.229 [-3.752, 2.168], loss: 1.900556, mean_absolute_error: 41.592751, mean_q: 83.716461\n"," 28971/80000: episode: 206, duration: 1.723s, episode steps: 208, steps per second: 121, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.197 [-1.488, 1.556], loss: 2.217752, mean_absolute_error: 41.113338, mean_q: 82.725288\n"," 29135/80000: episode: 207, duration: 1.366s, episode steps: 164, steps per second: 120, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.240 [-1.654, 0.922], loss: 2.296745, mean_absolute_error: 41.029606, mean_q: 82.528427\n"," 29322/80000: episode: 208, duration: 1.551s, episode steps: 187, steps per second: 121, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.351 [-1.075, 2.254], loss: 1.676301, mean_absolute_error: 41.119202, mean_q: 82.929886\n"," 29509/80000: episode: 209, duration: 1.553s, episode steps: 187, steps per second: 120, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.328 [-3.138, 1.227], loss: 1.718608, mean_absolute_error: 40.929642, mean_q: 82.382362\n"," 29648/80000: episode: 210, duration: 1.170s, episode steps: 139, steps per second: 119, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.263 [-1.644, 0.859], loss: 1.701668, mean_absolute_error: 40.416313, mean_q: 81.466347\n"," 29941/80000: episode: 211, duration: 2.425s, episode steps: 293, steps per second: 121, episode reward: 293.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.231 [-1.137, 2.415], loss: 1.533585, mean_absolute_error: 41.246246, mean_q: 83.041023\n"," 30102/80000: episode: 212, duration: 1.341s, episode steps: 161, steps per second: 120, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.377 [-3.177, 1.165], loss: 1.958565, mean_absolute_error: 41.089104, mean_q: 82.513199\n"," 30381/80000: episode: 213, duration: 2.299s, episode steps: 279, steps per second: 121, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.238 [-1.160, 2.210], loss: 1.462388, mean_absolute_error: 40.481274, mean_q: 81.501198\n"," 30554/80000: episode: 214, duration: 1.459s, episode steps: 173, steps per second: 119, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.358 [-2.786, 0.910], loss: 2.103423, mean_absolute_error: 40.484001, mean_q: 81.414330\n"," 30796/80000: episode: 215, duration: 1.996s, episode steps: 242, steps per second: 121, episode reward: 242.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.287 [-1.330, 2.412], loss: 1.293613, mean_absolute_error: 40.284729, mean_q: 81.143372\n"," 30954/80000: episode: 216, duration: 1.299s, episode steps: 158, steps per second: 122, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.225 [-1.436, 0.952], loss: 1.258451, mean_absolute_error: 40.826099, mean_q: 82.155586\n"," 31178/80000: episode: 217, duration: 1.868s, episode steps: 224, steps per second: 120, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.302 [-1.187, 2.406], loss: 1.662677, mean_absolute_error: 40.137505, mean_q: 80.693855\n"," 31452/80000: episode: 218, duration: 2.253s, episode steps: 274, steps per second: 122, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.259 [-1.344, 2.406], loss: 2.015788, mean_absolute_error: 40.048233, mean_q: 80.512894\n"," 31648/80000: episode: 219, duration: 1.639s, episode steps: 196, steps per second: 120, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.316 [-2.971, 1.112], loss: 1.777784, mean_absolute_error: 39.954964, mean_q: 80.394981\n"," 31828/80000: episode: 220, duration: 1.503s, episode steps: 180, steps per second: 120, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.220 [-1.808, 0.928], loss: 1.552186, mean_absolute_error: 40.299049, mean_q: 81.060478\n"," 32021/80000: episode: 221, duration: 1.605s, episode steps: 193, steps per second: 120, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.327 [-2.599, 1.062], loss: 1.040154, mean_absolute_error: 40.094837, mean_q: 80.795853\n"," 32230/80000: episode: 222, duration: 1.736s, episode steps: 209, steps per second: 120, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.321 [-1.293, 2.345], loss: 1.993022, mean_absolute_error: 39.969501, mean_q: 80.461121\n"," 32613/80000: episode: 223, duration: 3.178s, episode steps: 383, steps per second: 121, episode reward: 383.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.196 [-1.249, 2.418], loss: 1.349975, mean_absolute_error: 39.928577, mean_q: 80.444489\n"," 32776/80000: episode: 224, duration: 1.405s, episode steps: 163, steps per second: 116, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.215 [-1.484, 0.973], loss: 1.675652, mean_absolute_error: 39.307247, mean_q: 79.178780\n"," 32964/80000: episode: 225, duration: 1.604s, episode steps: 188, steps per second: 117, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.334 [-2.925, 1.106], loss: 1.513026, mean_absolute_error: 39.920372, mean_q: 80.454742\n"," 33154/80000: episode: 226, duration: 1.588s, episode steps: 190, steps per second: 120, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.336 [-2.436, 0.742], loss: 1.965000, mean_absolute_error: 39.750965, mean_q: 80.047859\n"," 33339/80000: episode: 227, duration: 1.560s, episode steps: 185, steps per second: 119, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.328 [-3.169, 1.403], loss: 1.286853, mean_absolute_error: 39.967186, mean_q: 80.557571\n"," 33559/80000: episode: 228, duration: 1.829s, episode steps: 220, steps per second: 120, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.298 [-2.431, 0.902], loss: 1.517823, mean_absolute_error: 39.897614, mean_q: 80.418343\n"," 33796/80000: episode: 229, duration: 1.965s, episode steps: 237, steps per second: 121, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.272 [-2.543, 0.933], loss: 1.639278, mean_absolute_error: 40.018303, mean_q: 80.590546\n"," 33996/80000: episode: 230, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.317 [-2.544, 0.987], loss: 1.472297, mean_absolute_error: 39.661415, mean_q: 79.964996\n"," 34234/80000: episode: 231, duration: 1.984s, episode steps: 238, steps per second: 120, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.272 [-2.970, 1.477], loss: 1.846100, mean_absolute_error: 40.101856, mean_q: 80.756775\n"," 34473/80000: episode: 232, duration: 1.996s, episode steps: 239, steps per second: 120, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.268 [-2.733, 1.293], loss: 1.077289, mean_absolute_error: 39.850559, mean_q: 80.347397\n"," 34663/80000: episode: 233, duration: 1.600s, episode steps: 190, steps per second: 119, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.333 [-2.565, 1.383], loss: 1.449558, mean_absolute_error: 40.639458, mean_q: 81.858521\n"," 34884/80000: episode: 234, duration: 1.837s, episode steps: 221, steps per second: 120, episode reward: 221.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.291 [-2.441, 1.014], loss: 1.789394, mean_absolute_error: 40.057896, mean_q: 80.671608\n"," 35137/80000: episode: 235, duration: 2.355s, episode steps: 253, steps per second: 107, episode reward: 253.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.253 [-2.586, 0.848], loss: 1.444348, mean_absolute_error: 40.133522, mean_q: 80.820702\n"," 35356/80000: episode: 236, duration: 2.170s, episode steps: 219, steps per second: 101, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.292 [-2.403, 1.109], loss: 2.425262, mean_absolute_error: 40.019283, mean_q: 80.493790\n"," 35715/80000: episode: 237, duration: 3.509s, episode steps: 359, steps per second: 102, episode reward: 359.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.177 [-2.433, 1.558], loss: 1.826531, mean_absolute_error: 39.884819, mean_q: 80.461708\n"," 35918/80000: episode: 238, duration: 2.000s, episode steps: 203, steps per second: 101, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.314 [-2.527, 0.799], loss: 1.148763, mean_absolute_error: 40.216331, mean_q: 81.162430\n"," 36170/80000: episode: 239, duration: 2.283s, episode steps: 252, steps per second: 110, episode reward: 252.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.253 [-2.422, 1.456], loss: 1.829116, mean_absolute_error: 40.300560, mean_q: 81.202179\n"," 36394/80000: episode: 240, duration: 1.863s, episode steps: 224, steps per second: 120, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.285 [-2.407, 0.791], loss: 2.408790, mean_absolute_error: 40.217247, mean_q: 80.979126\n"," 36584/80000: episode: 241, duration: 1.588s, episode steps: 190, steps per second: 120, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.343 [-2.446, 1.168], loss: 1.169378, mean_absolute_error: 40.389942, mean_q: 81.451897\n"," 36805/80000: episode: 242, duration: 1.864s, episode steps: 221, steps per second: 119, episode reward: 221.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.285 [-2.735, 1.115], loss: 2.067596, mean_absolute_error: 40.383205, mean_q: 81.352715\n"," 37007/80000: episode: 243, duration: 1.671s, episode steps: 202, steps per second: 121, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.316 [-2.573, 1.199], loss: 1.954989, mean_absolute_error: 39.917957, mean_q: 80.329582\n"," 37209/80000: episode: 244, duration: 1.687s, episode steps: 202, steps per second: 120, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.309 [-2.573, 1.068], loss: 1.810543, mean_absolute_error: 40.169838, mean_q: 80.993080\n"," 37709/80000: episode: 245, duration: 4.147s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.035 [-1.299, 1.365], loss: 1.374474, mean_absolute_error: 40.394447, mean_q: 81.519363\n"," 37936/80000: episode: 246, duration: 1.876s, episode steps: 227, steps per second: 121, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.285 [-2.576, 0.876], loss: 1.759531, mean_absolute_error: 40.934849, mean_q: 82.523163\n"," 38111/80000: episode: 247, duration: 1.461s, episode steps: 175, steps per second: 120, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.357 [-2.426, 0.977], loss: 1.168062, mean_absolute_error: 41.064735, mean_q: 82.805580\n"," 38611/80000: episode: 248, duration: 4.151s, episode steps: 500, steps per second: 120, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.028 [-1.260, 1.424], loss: 1.457240, mean_absolute_error: 40.759407, mean_q: 81.994164\n"," 39111/80000: episode: 249, duration: 4.206s, episode steps: 500, steps per second: 119, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.026 [-1.511, 1.347], loss: 1.389120, mean_absolute_error: 40.805805, mean_q: 82.116119\n"," 39270/80000: episode: 250, duration: 1.349s, episode steps: 159, steps per second: 118, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.365 [-2.783, 1.366], loss: 2.197367, mean_absolute_error: 40.572697, mean_q: 81.476990\n"," 39680/80000: episode: 251, duration: 3.407s, episode steps: 410, steps per second: 120, episode reward: 410.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.141 [-2.589, 1.562], loss: 2.862854, mean_absolute_error: 40.646412, mean_q: 81.643715\n"," 39859/80000: episode: 252, duration: 1.487s, episode steps: 179, steps per second: 120, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.345 [-2.724, 1.490], loss: 1.166544, mean_absolute_error: 40.946743, mean_q: 82.346161\n"," 40100/80000: episode: 253, duration: 1.993s, episode steps: 241, steps per second: 121, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.261 [-2.556, 1.003], loss: 1.266479, mean_absolute_error: 40.918488, mean_q: 82.368759\n"," 40247/80000: episode: 254, duration: 1.236s, episode steps: 147, steps per second: 119, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.418 [-2.426, 1.287], loss: 1.361212, mean_absolute_error: 40.552879, mean_q: 81.572342\n"," 40617/80000: episode: 255, duration: 3.061s, episode steps: 370, steps per second: 121, episode reward: 370.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.152 [-2.588, 1.368], loss: 1.653071, mean_absolute_error: 40.256969, mean_q: 81.032005\n"," 40872/80000: episode: 256, duration: 2.123s, episode steps: 255, steps per second: 120, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.246 [-2.595, 1.243], loss: 1.572036, mean_absolute_error: 40.353252, mean_q: 81.085571\n"," 41076/80000: episode: 257, duration: 1.711s, episode steps: 204, steps per second: 119, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.310 [-2.721, 1.330], loss: 1.238822, mean_absolute_error: 40.789124, mean_q: 82.073807\n"," 41235/80000: episode: 258, duration: 1.330s, episode steps: 159, steps per second: 120, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.381 [-2.773, 1.606], loss: 1.686263, mean_absolute_error: 40.550884, mean_q: 81.665115\n"," 41436/80000: episode: 259, duration: 1.671s, episode steps: 201, steps per second: 120, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.310 [-2.564, 1.070], loss: 3.789885, mean_absolute_error: 40.336964, mean_q: 81.039047\n"," 41725/80000: episode: 260, duration: 2.436s, episode steps: 289, steps per second: 119, episode reward: 289.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.214 [-2.403, 1.545], loss: 2.607370, mean_absolute_error: 40.418713, mean_q: 81.269402\n"," 41925/80000: episode: 261, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.305 [-2.532, 1.106], loss: 1.433225, mean_absolute_error: 40.563587, mean_q: 81.657433\n"," 42114/80000: episode: 262, duration: 1.576s, episode steps: 189, steps per second: 120, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.344 [-2.743, 1.249], loss: 1.235284, mean_absolute_error: 40.465801, mean_q: 81.531052\n"," 42362/80000: episode: 263, duration: 2.063s, episode steps: 248, steps per second: 120, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.241 [-2.521, 1.315], loss: 2.116155, mean_absolute_error: 40.522194, mean_q: 81.520950\n"," 42580/80000: episode: 264, duration: 1.822s, episode steps: 218, steps per second: 120, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.286 [-2.518, 1.015], loss: 1.432615, mean_absolute_error: 40.294544, mean_q: 81.174835\n"," 42763/80000: episode: 265, duration: 1.554s, episode steps: 183, steps per second: 118, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.334 [-2.542, 0.997], loss: 1.044105, mean_absolute_error: 39.898396, mean_q: 80.458504\n"," 42991/80000: episode: 266, duration: 1.922s, episode steps: 228, steps per second: 119, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.274 [-2.410, 1.455], loss: 0.941649, mean_absolute_error: 40.132336, mean_q: 80.802994\n"," 43171/80000: episode: 267, duration: 1.518s, episode steps: 180, steps per second: 119, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.340 [-2.562, 1.327], loss: 0.995354, mean_absolute_error: 40.338444, mean_q: 81.182449\n"," 43436/80000: episode: 268, duration: 2.234s, episode steps: 265, steps per second: 119, episode reward: 265.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.247 [-2.426, 1.506], loss: 1.488720, mean_absolute_error: 40.245281, mean_q: 80.959671\n"," 43654/80000: episode: 269, duration: 1.818s, episode steps: 218, steps per second: 120, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.304 [-2.431, 1.630], loss: 1.393358, mean_absolute_error: 40.578617, mean_q: 81.656876\n"," 43829/80000: episode: 270, duration: 1.475s, episode steps: 175, steps per second: 119, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.356 [-2.416, 1.120], loss: 0.988043, mean_absolute_error: 40.561954, mean_q: 81.674835\n"," 44009/80000: episode: 271, duration: 1.515s, episode steps: 180, steps per second: 119, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.334 [-2.566, 1.112], loss: 1.296649, mean_absolute_error: 40.505856, mean_q: 81.495407\n"," 44283/80000: episode: 272, duration: 2.315s, episode steps: 274, steps per second: 118, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.226 [-2.538, 1.647], loss: 0.892039, mean_absolute_error: 40.384937, mean_q: 81.238930\n"," 44506/80000: episode: 273, duration: 1.867s, episode steps: 223, steps per second: 119, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.293 [-2.416, 1.665], loss: 0.783559, mean_absolute_error: 40.421707, mean_q: 81.299454\n"," 44767/80000: episode: 274, duration: 2.186s, episode steps: 261, steps per second: 119, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.249 [-2.400, 1.257], loss: 1.357871, mean_absolute_error: 40.621777, mean_q: 81.812447\n"," 44986/80000: episode: 275, duration: 1.844s, episode steps: 219, steps per second: 119, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.284 [-2.402, 1.602], loss: 0.916910, mean_absolute_error: 40.926197, mean_q: 82.369637\n"," 45165/80000: episode: 276, duration: 1.501s, episode steps: 179, steps per second: 119, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.344 [-2.416, 1.410], loss: 0.916918, mean_absolute_error: 41.328529, mean_q: 83.161774\n"," 45362/80000: episode: 277, duration: 1.656s, episode steps: 197, steps per second: 119, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.323 [-2.430, 1.641], loss: 1.172863, mean_absolute_error: 41.166355, mean_q: 82.789070\n"," 45542/80000: episode: 278, duration: 1.498s, episode steps: 180, steps per second: 120, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.347 [-2.438, 1.757], loss: 1.728286, mean_absolute_error: 41.457546, mean_q: 83.383575\n"," 45882/80000: episode: 279, duration: 2.850s, episode steps: 340, steps per second: 119, episode reward: 340.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.195 [-2.553, 1.419], loss: 1.350219, mean_absolute_error: 40.965801, mean_q: 82.515236\n"," 46155/80000: episode: 280, duration: 2.280s, episode steps: 273, steps per second: 120, episode reward: 273.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.237 [-2.714, 1.316], loss: 2.078393, mean_absolute_error: 41.952225, mean_q: 84.440628\n"," 46382/80000: episode: 281, duration: 1.904s, episode steps: 227, steps per second: 119, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.285 [-2.407, 1.596], loss: 0.857656, mean_absolute_error: 41.425640, mean_q: 83.527000\n"," 46644/80000: episode: 282, duration: 2.155s, episode steps: 262, steps per second: 122, episode reward: 262.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.252 [-2.912, 1.147], loss: 0.773189, mean_absolute_error: 41.639072, mean_q: 83.953674\n"," 46877/80000: episode: 283, duration: 1.923s, episode steps: 233, steps per second: 121, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.282 [-2.428, 1.588], loss: 2.062441, mean_absolute_error: 41.733200, mean_q: 84.061630\n"," 47377/80000: episode: 284, duration: 4.127s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.021 [-1.602, 1.460], loss: 1.377766, mean_absolute_error: 42.530556, mean_q: 85.696487\n"," 47877/80000: episode: 285, duration: 4.147s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.048 [-1.669, 1.745], loss: 1.779410, mean_absolute_error: 42.596046, mean_q: 85.789528\n"," 48190/80000: episode: 286, duration: 2.636s, episode steps: 313, steps per second: 119, episode reward: 313.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.237 [-2.467, 1.383], loss: 1.898986, mean_absolute_error: 43.011532, mean_q: 86.659225\n"," 48690/80000: episode: 287, duration: 4.154s, episode steps: 500, steps per second: 120, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.042 [-1.552, 1.685], loss: 1.451048, mean_absolute_error: 43.242050, mean_q: 87.137543\n"," 49190/80000: episode: 288, duration: 4.125s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.037 [-1.673, 1.811], loss: 2.689067, mean_absolute_error: 43.811047, mean_q: 88.296219\n"," 49690/80000: episode: 289, duration: 4.128s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.062 [-1.633, 1.528], loss: 3.329362, mean_absolute_error: 44.414730, mean_q: 89.502922\n"," 50190/80000: episode: 290, duration: 4.127s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.104 [-1.645, 1.862], loss: 2.648972, mean_absolute_error: 44.666451, mean_q: 89.980164\n"," 50690/80000: episode: 291, duration: 4.101s, episode steps: 500, steps per second: 122, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-1.833, 2.053], loss: 3.538613, mean_absolute_error: 45.292126, mean_q: 91.228958\n"," 50851/80000: episode: 292, duration: 1.305s, episode steps: 161, steps per second: 123, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.442 [-1.626, 2.418], loss: 2.250192, mean_absolute_error: 45.552109, mean_q: 91.849014\n"," 51351/80000: episode: 293, duration: 4.152s, episode steps: 500, steps per second: 120, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.166 [-1.672, 1.735], loss: 2.753380, mean_absolute_error: 45.646374, mean_q: 91.892517\n"," 51540/80000: episode: 294, duration: 1.586s, episode steps: 189, steps per second: 119, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.392 [-1.426, 2.411], loss: 3.234062, mean_absolute_error: 45.787594, mean_q: 92.175293\n"," 51705/80000: episode: 295, duration: 1.363s, episode steps: 165, steps per second: 121, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.426 [-1.256, 2.415], loss: 1.081926, mean_absolute_error: 46.166759, mean_q: 93.003754\n"," 51917/80000: episode: 296, duration: 1.729s, episode steps: 212, steps per second: 123, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.364 [-2.010, 2.547], loss: 2.582014, mean_absolute_error: 46.695896, mean_q: 93.905327\n"," 52080/80000: episode: 297, duration: 1.307s, episode steps: 163, steps per second: 125, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.400 [-1.617, 2.437], loss: 3.923680, mean_absolute_error: 46.299480, mean_q: 93.180511\n"," 52231/80000: episode: 298, duration: 1.249s, episode steps: 151, steps per second: 121, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.441 [-1.143, 2.558], loss: 2.524390, mean_absolute_error: 46.676510, mean_q: 93.886528\n"," 52408/80000: episode: 299, duration: 1.439s, episode steps: 177, steps per second: 123, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.403 [-1.782, 2.563], loss: 2.174278, mean_absolute_error: 46.928120, mean_q: 94.498962\n"," 52554/80000: episode: 300, duration: 1.185s, episode steps: 146, steps per second: 123, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.462 [-1.109, 2.763], loss: 2.439000, mean_absolute_error: 46.985424, mean_q: 94.418137\n"," 52720/80000: episode: 301, duration: 1.353s, episode steps: 166, steps per second: 123, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.408 [-1.558, 2.751], loss: 1.670625, mean_absolute_error: 47.279778, mean_q: 95.119545\n"," 53097/80000: episode: 302, duration: 3.060s, episode steps: 377, steps per second: 123, episode reward: 377.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.246 [-2.269, 1.759], loss: 2.980131, mean_absolute_error: 47.291054, mean_q: 95.028679\n"," 53321/80000: episode: 303, duration: 1.812s, episode steps: 224, steps per second: 124, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.290 [-1.471, 2.211], loss: 1.809016, mean_absolute_error: 47.448292, mean_q: 95.370033\n"," 53513/80000: episode: 304, duration: 1.606s, episode steps: 192, steps per second: 120, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.352 [-1.917, 3.385], loss: 2.123450, mean_absolute_error: 47.453491, mean_q: 95.454651\n"," 53661/80000: episode: 305, duration: 1.235s, episode steps: 148, steps per second: 120, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.374 [-1.257, 1.991], loss: 4.262005, mean_absolute_error: 47.589844, mean_q: 95.626945\n"," 53865/80000: episode: 306, duration: 1.666s, episode steps: 204, steps per second: 122, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.285 [-1.415, 2.028], loss: 0.730243, mean_absolute_error: 47.301273, mean_q: 95.200897\n"," 54032/80000: episode: 307, duration: 1.361s, episode steps: 167, steps per second: 123, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.316 [-1.166, 2.024], loss: 0.727544, mean_absolute_error: 47.400120, mean_q: 95.483139\n"," 54273/80000: episode: 308, duration: 1.981s, episode steps: 241, steps per second: 122, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.245 [-1.851, 1.877], loss: 0.702969, mean_absolute_error: 47.943253, mean_q: 96.509033\n"," 54423/80000: episode: 309, duration: 1.240s, episode steps: 150, steps per second: 121, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.417 [-1.119, 2.762], loss: 0.644718, mean_absolute_error: 47.950989, mean_q: 96.572731\n"," 54622/80000: episode: 310, duration: 1.636s, episode steps: 199, steps per second: 122, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.313 [-1.193, 2.742], loss: 2.367288, mean_absolute_error: 48.322117, mean_q: 97.215843\n"," 54764/80000: episode: 311, duration: 1.152s, episode steps: 142, steps per second: 123, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.434 [-1.259, 3.127], loss: 4.980559, mean_absolute_error: 48.175659, mean_q: 96.865898\n"," 54909/80000: episode: 312, duration: 1.189s, episode steps: 145, steps per second: 122, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.435 [-1.672, 2.425], loss: 4.928444, mean_absolute_error: 47.926739, mean_q: 96.345818\n"," 55092/80000: episode: 313, duration: 1.532s, episode steps: 183, steps per second: 119, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.361 [-1.488, 2.740], loss: 0.850866, mean_absolute_error: 48.040974, mean_q: 96.684547\n"," 55269/80000: episode: 314, duration: 1.769s, episode steps: 177, steps per second: 100, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.099 [-1.486, 1.400], loss: 2.665538, mean_absolute_error: 47.948944, mean_q: 96.348579\n"," 55509/80000: episode: 315, duration: 2.391s, episode steps: 240, steps per second: 100, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.282 [-1.618, 2.804], loss: 3.049390, mean_absolute_error: 48.043484, mean_q: 96.535614\n"," 55721/80000: episode: 316, duration: 1.888s, episode steps: 212, steps per second: 112, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.328 [-1.887, 2.606], loss: 2.748757, mean_absolute_error: 47.491516, mean_q: 95.453049\n"," 55953/80000: episode: 317, duration: 1.884s, episode steps: 232, steps per second: 123, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.308 [-1.123, 2.596], loss: 2.359490, mean_absolute_error: 47.818886, mean_q: 96.146881\n"," 56211/80000: episode: 318, duration: 2.089s, episode steps: 258, steps per second: 123, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.309 [-1.611, 2.589], loss: 0.688598, mean_absolute_error: 47.565575, mean_q: 95.794540\n"," 56489/80000: episode: 319, duration: 2.256s, episode steps: 278, steps per second: 123, episode reward: 278.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.264 [-2.013, 2.789], loss: 3.325328, mean_absolute_error: 47.866840, mean_q: 96.230591\n"," 56652/80000: episode: 320, duration: 1.302s, episode steps: 163, steps per second: 125, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.279 [-1.304, 1.847], loss: 0.914299, mean_absolute_error: 47.571457, mean_q: 95.719780\n"," 57020/80000: episode: 321, duration: 2.961s, episode steps: 368, steps per second: 124, episode reward: 368.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.136 [-1.614, 1.752], loss: 0.540228, mean_absolute_error: 47.997135, mean_q: 96.615913\n"," 57275/80000: episode: 322, duration: 2.069s, episode steps: 255, steps per second: 123, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.269 [-1.552, 2.763], loss: 2.447557, mean_absolute_error: 48.063946, mean_q: 96.640610\n"," 57456/80000: episode: 323, duration: 1.470s, episode steps: 181, steps per second: 123, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.238 [-1.111, 1.644], loss: 0.645034, mean_absolute_error: 47.823559, mean_q: 96.149734\n"," 57756/80000: episode: 324, duration: 2.410s, episode steps: 300, steps per second: 124, episode reward: 300.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.246 [-1.680, 2.411], loss: 1.781320, mean_absolute_error: 48.428486, mean_q: 97.312706\n"," 57918/80000: episode: 325, duration: 1.307s, episode steps: 162, steps per second: 124, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.080 [-1.737, 2.027], loss: 3.712237, mean_absolute_error: 47.891731, mean_q: 96.143463\n"," 58056/80000: episode: 326, duration: 1.109s, episode steps: 138, steps per second: 124, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.456 [-1.710, 2.847], loss: 0.457297, mean_absolute_error: 48.625065, mean_q: 97.745529\n"," 58308/80000: episode: 327, duration: 2.013s, episode steps: 252, steps per second: 125, episode reward: 252.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.306 [-1.879, 2.406], loss: 1.589126, mean_absolute_error: 47.752068, mean_q: 95.876945\n"," 58501/80000: episode: 328, duration: 1.525s, episode steps: 193, steps per second: 127, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.023 [-1.901, 2.419], loss: 2.056684, mean_absolute_error: 47.686913, mean_q: 95.699059\n"," 58643/80000: episode: 329, duration: 1.152s, episode steps: 142, steps per second: 123, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.023 [-1.777, 2.137], loss: 0.521903, mean_absolute_error: 47.188347, mean_q: 94.745781\n"," 58829/80000: episode: 330, duration: 1.512s, episode steps: 186, steps per second: 123, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.366 [-1.149, 2.408], loss: 3.472079, mean_absolute_error: 47.326149, mean_q: 94.848640\n"," 58927/80000: episode: 331, duration: 0.801s, episode steps: 98, steps per second: 122, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.031 [-1.574, 1.669], loss: 3.089586, mean_absolute_error: 47.894894, mean_q: 95.981079\n"," 58994/80000: episode: 332, duration: 0.551s, episode steps: 67, steps per second: 122, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.000 [-1.591, 1.907], loss: 0.611929, mean_absolute_error: 47.173679, mean_q: 94.658699\n"," 59494/80000: episode: 333, duration: 4.055s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.001 [-2.000, 2.064], loss: 3.648858, mean_absolute_error: 47.671913, mean_q: 95.612740\n"," 59890/80000: episode: 334, duration: 3.209s, episode steps: 396, steps per second: 123, episode reward: 396.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.167 [-1.573, 2.083], loss: 4.490757, mean_absolute_error: 47.514160, mean_q: 95.330276\n"," 59961/80000: episode: 335, duration: 0.605s, episode steps: 71, steps per second: 117, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.023 [-1.932, 1.734], loss: 4.418353, mean_absolute_error: 47.755512, mean_q: 95.689072\n"," 60211/80000: episode: 336, duration: 2.032s, episode steps: 250, steps per second: 123, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.174 [-1.801, 1.720], loss: 5.612525, mean_absolute_error: 47.202133, mean_q: 94.597374\n"," 60711/80000: episode: 337, duration: 4.077s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.004 [-1.815, 2.113], loss: 3.579150, mean_absolute_error: 47.053802, mean_q: 94.434456\n"," 61001/80000: episode: 338, duration: 2.370s, episode steps: 290, steps per second: 122, episode reward: 290.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.039 [-1.721, 1.575], loss: 2.439440, mean_absolute_error: 47.386234, mean_q: 95.161568\n"," 61501/80000: episode: 339, duration: 4.052s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-2.199, 2.040], loss: 3.182510, mean_absolute_error: 47.503750, mean_q: 95.394623\n"," 62001/80000: episode: 340, duration: 4.060s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.982, 2.062], loss: 3.010911, mean_absolute_error: 47.742702, mean_q: 96.012238\n"," 62467/80000: episode: 341, duration: 3.815s, episode steps: 466, steps per second: 122, episode reward: 466.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.028 [-3.017, 2.351], loss: 4.858103, mean_absolute_error: 47.886719, mean_q: 96.234703\n"," 62967/80000: episode: 342, duration: 4.018s, episode steps: 500, steps per second: 124, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.055 [-1.778, 1.966], loss: 4.206728, mean_absolute_error: 48.170204, mean_q: 96.849785\n"," 63086/80000: episode: 343, duration: 0.973s, episode steps: 119, steps per second: 122, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.555 [0.000, 1.000], mean observation: 0.070 [-2.870, 2.495], loss: 7.118299, mean_absolute_error: 48.355816, mean_q: 97.108688\n"," 63586/80000: episode: 344, duration: 4.043s, episode steps: 500, steps per second: 124, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.035 [-1.615, 1.641], loss: 4.223017, mean_absolute_error: 48.603092, mean_q: 97.672112\n"," 64086/80000: episode: 345, duration: 4.017s, episode steps: 500, steps per second: 124, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.042 [-1.457, 1.334], loss: 3.751346, mean_absolute_error: 48.661522, mean_q: 97.891922\n"," 64330/80000: episode: 346, duration: 1.992s, episode steps: 244, steps per second: 123, episode reward: 244.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.020 [-1.338, 1.498], loss: 2.949095, mean_absolute_error: 49.195400, mean_q: 98.950134\n"," 64830/80000: episode: 347, duration: 4.137s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.000 [-1.458, 1.721], loss: 2.007633, mean_absolute_error: 49.704727, mean_q: 100.031731\n"," 65330/80000: episode: 348, duration: 4.001s, episode steps: 500, steps per second: 125, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.325, 1.323], loss: 3.140590, mean_absolute_error: 50.337929, mean_q: 101.241356\n"," 65830/80000: episode: 349, duration: 3.981s, episode steps: 500, steps per second: 126, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.530, 1.753], loss: 3.540313, mean_absolute_error: 51.119732, mean_q: 102.874252\n"," 66330/80000: episode: 350, duration: 4.011s, episode steps: 500, steps per second: 125, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.107 [-1.661, 1.428], loss: 6.351820, mean_absolute_error: 52.135376, mean_q: 104.877785\n"," 66830/80000: episode: 351, duration: 4.011s, episode steps: 500, steps per second: 125, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.127 [-1.685, 1.351], loss: 3.391061, mean_absolute_error: 52.956375, mean_q: 106.750534\n"," 67330/80000: episode: 352, duration: 3.982s, episode steps: 500, steps per second: 126, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.144 [-1.721, 1.389], loss: 6.167873, mean_absolute_error: 54.018250, mean_q: 108.747299\n"," 67830/80000: episode: 353, duration: 4.015s, episode steps: 500, steps per second: 125, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.171 [-2.101, 1.565], loss: 6.931195, mean_absolute_error: 55.590725, mean_q: 112.144531\n"," 68330/80000: episode: 354, duration: 4.000s, episode steps: 500, steps per second: 125, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.179 [-1.730, 1.112], loss: 4.219805, mean_absolute_error: 57.076805, mean_q: 115.405136\n"," 68830/80000: episode: 355, duration: 4.087s, episode steps: 500, steps per second: 122, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: -0.196 [-1.737, 1.183], loss: 5.755243, mean_absolute_error: 59.097744, mean_q: 119.480156\n"," 69330/80000: episode: 356, duration: 4.020s, episode steps: 500, steps per second: 124, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.217 [-2.103, 1.378], loss: 8.843658, mean_absolute_error: 60.892101, mean_q: 122.996155\n"," 69830/80000: episode: 357, duration: 4.021s, episode steps: 500, steps per second: 124, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.213 [-1.678, 1.420], loss: 7.210423, mean_absolute_error: 62.677048, mean_q: 126.556076\n"," 70330/80000: episode: 358, duration: 4.049s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.019 [-1.706, 1.881], loss: 10.757073, mean_absolute_error: 63.762569, mean_q: 128.700562\n"," 70696/80000: episode: 359, duration: 2.972s, episode steps: 366, steps per second: 123, episode reward: 366.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.734, 1.751], loss: 5.119740, mean_absolute_error: 64.796738, mean_q: 130.822418\n"," 71196/80000: episode: 360, duration: 4.062s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-1.673, 1.412], loss: 9.663694, mean_absolute_error: 65.526901, mean_q: 132.183319\n"," 71677/80000: episode: 361, duration: 3.879s, episode steps: 481, steps per second: 124, episode reward: 481.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.180 [-2.209, 1.489], loss: 15.137214, mean_absolute_error: 66.078125, mean_q: 132.917084\n"," 72088/80000: episode: 362, duration: 3.384s, episode steps: 411, steps per second: 121, episode reward: 411.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.127 [-2.268, 1.295], loss: 11.715622, mean_absolute_error: 66.095131, mean_q: 132.800232\n"," 72238/80000: episode: 363, duration: 1.241s, episode steps: 150, steps per second: 121, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.395 [-2.035, 1.315], loss: 4.406497, mean_absolute_error: 66.425911, mean_q: 133.572708\n"," 72395/80000: episode: 364, duration: 1.291s, episode steps: 157, steps per second: 122, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.333 [-2.068, 1.307], loss: 7.820787, mean_absolute_error: 66.224358, mean_q: 133.035904\n"," 72496/80000: episode: 365, duration: 0.841s, episode steps: 101, steps per second: 120, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.495 [-2.124, 0.956], loss: 10.341522, mean_absolute_error: 66.333992, mean_q: 132.866714\n"," 72573/80000: episode: 366, duration: 0.761s, episode steps: 77, steps per second: 101, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.189 [-1.979, 1.478], loss: 22.569630, mean_absolute_error: 66.698273, mean_q: 133.443481\n"," 72669/80000: episode: 367, duration: 0.957s, episode steps: 96, steps per second: 100, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.544 [-2.238, 0.957], loss: 15.817082, mean_absolute_error: 66.298286, mean_q: 132.661423\n"," 72799/80000: episode: 368, duration: 1.283s, episode steps: 130, steps per second: 101, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.438 [-2.142, 1.383], loss: 11.944527, mean_absolute_error: 66.102585, mean_q: 132.610733\n"," 73064/80000: episode: 369, duration: 2.623s, episode steps: 265, steps per second: 101, episode reward: 265.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.157 [-2.345, 1.357], loss: 9.166953, mean_absolute_error: 66.176613, mean_q: 132.747620\n"," 73175/80000: episode: 370, duration: 1.099s, episode steps: 111, steps per second: 101, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.530 [-2.441, 1.005], loss: 13.580044, mean_absolute_error: 66.420692, mean_q: 133.409424\n"," 73322/80000: episode: 371, duration: 1.443s, episode steps: 147, steps per second: 102, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.425 [-2.341, 1.597], loss: 13.243174, mean_absolute_error: 66.234146, mean_q: 132.875824\n"," 73426/80000: episode: 372, duration: 1.031s, episode steps: 104, steps per second: 101, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.545 [-2.364, 0.734], loss: 14.103680, mean_absolute_error: 65.984550, mean_q: 132.414917\n"," 73531/80000: episode: 373, duration: 1.006s, episode steps: 105, steps per second: 104, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.544 [-2.344, 0.888], loss: 5.787301, mean_absolute_error: 66.526634, mean_q: 133.793732\n"," 73647/80000: episode: 374, duration: 0.943s, episode steps: 116, steps per second: 123, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.511 [-2.435, 0.972], loss: 14.958976, mean_absolute_error: 67.247086, mean_q: 135.022018\n"," 73767/80000: episode: 375, duration: 0.985s, episode steps: 120, steps per second: 122, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.518 [-2.294, 1.267], loss: 16.103838, mean_absolute_error: 67.178108, mean_q: 134.562469\n"," 73922/80000: episode: 376, duration: 1.246s, episode steps: 155, steps per second: 124, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.441 [-2.260, 1.512], loss: 14.288707, mean_absolute_error: 66.968224, mean_q: 134.469025\n"," 74046/80000: episode: 377, duration: 0.998s, episode steps: 124, steps per second: 124, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.490 [-2.238, 1.352], loss: 5.647861, mean_absolute_error: 66.973534, mean_q: 134.548065\n"," 74161/80000: episode: 378, duration: 0.946s, episode steps: 115, steps per second: 122, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.519 [-2.347, 1.234], loss: 16.306389, mean_absolute_error: 66.805145, mean_q: 134.129227\n"," 74274/80000: episode: 379, duration: 0.924s, episode steps: 113, steps per second: 122, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.494 [-2.285, 1.315], loss: 7.041574, mean_absolute_error: 67.423233, mean_q: 135.476608\n"," 74396/80000: episode: 380, duration: 0.994s, episode steps: 122, steps per second: 123, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.505 [-2.310, 1.365], loss: 7.492861, mean_absolute_error: 67.035500, mean_q: 134.889145\n"," 74513/80000: episode: 381, duration: 0.950s, episode steps: 117, steps per second: 123, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.504 [-2.270, 1.255], loss: 15.166004, mean_absolute_error: 67.420364, mean_q: 135.526855\n"," 74627/80000: episode: 382, duration: 0.932s, episode steps: 114, steps per second: 122, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.479 [-2.241, 1.163], loss: 9.616778, mean_absolute_error: 67.647171, mean_q: 136.225876\n"," 74758/80000: episode: 383, duration: 1.071s, episode steps: 131, steps per second: 122, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.479 [-2.230, 1.204], loss: 1.641167, mean_absolute_error: 67.848488, mean_q: 136.629211\n"," 74910/80000: episode: 384, duration: 1.245s, episode steps: 152, steps per second: 122, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.438 [-2.299, 1.448], loss: 11.281816, mean_absolute_error: 68.007538, mean_q: 136.637604\n"," 75017/80000: episode: 385, duration: 0.878s, episode steps: 107, steps per second: 122, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.543 [-3.186, 1.256], loss: 5.475319, mean_absolute_error: 67.386238, mean_q: 135.629456\n"," 75146/80000: episode: 386, duration: 1.042s, episode steps: 129, steps per second: 124, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.474 [-3.121, 1.654], loss: 9.932367, mean_absolute_error: 68.121384, mean_q: 136.886322\n"," 75254/80000: episode: 387, duration: 0.889s, episode steps: 108, steps per second: 121, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.468 [-2.236, 1.455], loss: 9.300247, mean_absolute_error: 68.065002, mean_q: 136.850311\n"," 75389/80000: episode: 388, duration: 1.108s, episode steps: 135, steps per second: 122, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.453 [-2.163, 1.489], loss: 9.899047, mean_absolute_error: 67.796394, mean_q: 136.298721\n"," 75498/80000: episode: 389, duration: 0.899s, episode steps: 109, steps per second: 121, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.413 [0.000, 1.000], mean observation: -0.512 [-3.507, 1.556], loss: 11.566752, mean_absolute_error: 68.318062, mean_q: 137.398712\n"," 75642/80000: episode: 390, duration: 1.188s, episode steps: 144, steps per second: 121, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.454 [-2.251, 1.008], loss: 19.208523, mean_absolute_error: 68.436729, mean_q: 137.311096\n"," 75758/80000: episode: 391, duration: 0.973s, episode steps: 116, steps per second: 119, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.518 [-2.941, 1.450], loss: 16.417427, mean_absolute_error: 68.529221, mean_q: 137.494278\n"," 75873/80000: episode: 392, duration: 0.937s, episode steps: 115, steps per second: 123, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.497 [-3.171, 1.284], loss: 1.684188, mean_absolute_error: 68.181770, mean_q: 137.191833\n"," 76026/80000: episode: 393, duration: 1.254s, episode steps: 153, steps per second: 122, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.432 [-3.130, 1.564], loss: 15.953942, mean_absolute_error: 68.128632, mean_q: 136.671539\n"," 76147/80000: episode: 394, duration: 1.010s, episode steps: 121, steps per second: 120, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.475 [-3.500, 1.789], loss: 4.502866, mean_absolute_error: 67.920639, mean_q: 136.595810\n"," 76281/80000: episode: 395, duration: 1.105s, episode steps: 134, steps per second: 121, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.437 [-2.021, 1.273], loss: 22.073370, mean_absolute_error: 68.387100, mean_q: 137.104538\n"," 76401/80000: episode: 396, duration: 0.989s, episode steps: 120, steps per second: 121, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.469 [-2.123, 1.201], loss: 3.688427, mean_absolute_error: 67.911934, mean_q: 136.625290\n"," 76526/80000: episode: 397, duration: 1.018s, episode steps: 125, steps per second: 123, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.450 [-2.066, 1.365], loss: 12.909166, mean_absolute_error: 68.276245, mean_q: 136.959381\n"," 76678/80000: episode: 398, duration: 1.264s, episode steps: 152, steps per second: 120, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.373 [-1.868, 1.195], loss: 11.091743, mean_absolute_error: 67.883171, mean_q: 136.304352\n"," 76870/80000: episode: 399, duration: 1.589s, episode steps: 192, steps per second: 121, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.338 [-1.898, 1.290], loss: 5.689906, mean_absolute_error: 67.958809, mean_q: 136.598511\n"," 77005/80000: episode: 400, duration: 1.115s, episode steps: 135, steps per second: 121, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.443 [-2.045, 1.234], loss: 1.175009, mean_absolute_error: 67.841202, mean_q: 136.522888\n"," 77154/80000: episode: 401, duration: 1.222s, episode steps: 149, steps per second: 122, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.400 [-2.024, 1.206], loss: 10.619719, mean_absolute_error: 68.147888, mean_q: 136.762527\n"," 77329/80000: episode: 402, duration: 1.426s, episode steps: 175, steps per second: 123, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.392 [-2.059, 1.159], loss: 9.220562, mean_absolute_error: 67.679863, mean_q: 135.866837\n"," 77481/80000: episode: 403, duration: 1.244s, episode steps: 152, steps per second: 122, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.417 [-2.021, 0.992], loss: 9.962143, mean_absolute_error: 67.717606, mean_q: 135.916214\n"," 77744/80000: episode: 404, duration: 2.110s, episode steps: 263, steps per second: 125, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.351 [-2.013, 1.459], loss: 12.035349, mean_absolute_error: 67.263741, mean_q: 135.014816\n"," 77847/80000: episode: 405, duration: 0.864s, episode steps: 103, steps per second: 119, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.145 [-1.720, 1.716], loss: 11.227798, mean_absolute_error: 67.756477, mean_q: 135.790955\n"," 78186/80000: episode: 406, duration: 2.779s, episode steps: 339, steps per second: 122, episode reward: 339.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.354 [-1.967, 1.280], loss: 14.510555, mean_absolute_error: 67.509865, mean_q: 135.199966\n"," 78423/80000: episode: 407, duration: 1.934s, episode steps: 237, steps per second: 123, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.376 [-1.970, 1.325], loss: 4.380217, mean_absolute_error: 67.194565, mean_q: 134.952713\n"," 78577/80000: episode: 408, duration: 1.254s, episode steps: 154, steps per second: 123, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.421 [-2.026, 1.316], loss: 14.613221, mean_absolute_error: 67.450592, mean_q: 135.285492\n"," 79077/80000: episode: 409, duration: 4.074s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.231 [-1.727, 1.528], loss: 15.659102, mean_absolute_error: 66.962265, mean_q: 134.187912\n"," 79577/80000: episode: 410, duration: 4.067s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.231 [-1.726, 1.768], loss: 11.233654, mean_absolute_error: 66.503677, mean_q: 133.408249\n","done, took 671.560 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"df9FoIHJdPzE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"ea552f4b-0dd2-46c8-e45e-a685e53ed05e","executionInfo":{"status":"ok","timestamp":1548696686333,"user_tz":-330,"elapsed":25158,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"UUlC1Lrig6w5","colab_type":"code","colab":{}},"cell_type":"code","source":["os.chdir('/content/drive/My Drive/open_ai/models')\n","filename = 'cart_pole_dqn_boltmann_sequential.h5'\n","\n","dqn.save_weights(filename, overwrite=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xzPPHpWPhKz1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"a65eb94a-bd44-485b-a8cd-17a5c717b1b0","executionInfo":{"status":"ok","timestamp":1548696909696,"user_tz":-330,"elapsed":3015,"user":{"displayName":"Vaibhav Gupta","photoUrl":"","userId":"07285709977412521631"}}},"cell_type":"code","source":["dqn.test(env, nb_episodes=5, visualize=False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Testing for 5 episodes ...\n","Episode 1: reward: 500.000, steps: 500\n","Episode 2: reward: 500.000, steps: 500\n","Episode 3: reward: 500.000, steps: 500\n","Episode 4: reward: 500.000, steps: 500\n","Episode 5: reward: 500.000, steps: 500\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4bd3d8da58>"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"VVTASzFDh0Vp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}